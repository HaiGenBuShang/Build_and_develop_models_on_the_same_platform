##
## Necessary functions to train AIMS models.
##
## Author : Eric R. Paquet (eric.r.paquet@gmail.com)
##
## Copyright McGill University, 2015
##

source("reproduce.apam50.paper.part1.toolbox.R")
source("hc.tsp.R")
require(gplots)

## functio to select the optimal number of rules
selOptimalDefault <- function(cv.stats){
  which.max(cv.stats$test.mean$ALL)
}

## Update 
trainAIMS_2 <- function(D,cl,EntrezID,
                      PREFIX="",
                      w=rep(1,length(cl)),
                      k.fold=20,
                      num.of.rules=seq(1,50,1),
                      train.function=one.vs.all.tsp,
                      pred.function=predict.one.vs.all.tsp,
                      sel.optimal.func=selOptimalDefault){
  ## Will train an AIMS model to recapitulate the assignments in cl using the
  ## dataset D.
  ##
  ## D = a raw gene expression dataset row = genes, column = samples
  ## cl = the classes you want to train eg. "Basal","Her2","LumA","LumB",
  ##      "Normal" or "low","intermediate","high"
  ## EntrezID = gene names to use in your model. Note they have to be unique, 
  ##            but this could also be symbols
  ## PREFIX = All the outputs generated by this script will be prefixed by 
  ##           PREFIX
  ## w = You should use the weighted version. use getWeights to get the 
  ##     weights. The reason to use weights is to down-weighted the large 
  ##     datasets and up-weighted the smaller dataset so all datasets
  ##     contribute equaly to the rules selection
  ## k.fold = How many splits in the k-fold cross validation
  ## num.of.rules = number of rules to select. WARNING should be <= 100
  ## train.function = look in hc.tsp.R for other training function.
  ##                  Default = one.vs.all.tsp
  ## pred.function = look in hc.tsp.R for other training function.
  ##                 Default = predict.one.vs.all.tsp
  ##
  ## Output:
  ##
  ## PREFIX.CV.kf.num.features.pdf : the cross-validation plots justifying the
  ##                                 selection for the k rules
  ## cv.optimal.k.stats.xls : Overall and subtype-specific agreements estimated
  ##                          from the cross-validation
  ## AIMS.GS.RData: the trained model
  ## heatmap.nbc.pdf: heatmap displaying the different rules
  ##
  
  ## Sanity check
  stopifnot(ncol(D) == length(cl))
  stopifnot(ncol(D) == length(w))
  stopifnot(k.fold > 1 & k.fold < ncol(D)/2) ## the ncol(D)/2 is a bit arbitrary here, but it is just to prevent some crazy k.fold settings
  stopifnot(all(num.of.rules >= 1 & num.of.rules <= nrow(D)))
  
  
  output.tag <- sprintf("%s.CV.kf.%d.num.features.%d",PREFIX,k.fold,max(num.of.rules))
    
  cv <- cv.hctsp(D=D, ## gene expression dataset
                 cl=cl, ## claudin-low.assignment
                 GeneName=EntrezID, ## Entrez gene ids
                 k.fold=k.fold, ## What is the K of the K-fold cross-validation
                 w=w, ## This is a weighted TSP
                 k=num.of.rules, ## How many simple rules per subtype to consider 
                 train.func=train.function, ## What is the training function see hc.tsp.R
                 pred.func=pred.function) ## What is the prediction function hc.tsp.R
  
  print("finished CV!")
  ## save cross-validation results to save a couple of time in the future
  save(cv,file=sprintf("%s.RData",output.tag))
  
  pdf(sprintf("%s.pdf",output.tag))
  cv.stats <- cv.plot(cv)
  dev.off()

  optimal.k <- num.of.rules[sel.optimal.func(cv.stats)] ## which.max(cv.stats$test.mean$ALL) ## This will pick the optimal number of features

  ## generate some stats from the cross-validation ie subtype-specific agreements +- 95% CI
  subtypes.stats.optimal <- c()
  for (si in names(cv.stats$test.mean)){
    m.cur <- cv.stats$test.mean[[si]][optimal.k]
    ic.c <- 1.96*cv.stats$test.sd[[si]][optimal.k]/sqrt(length(cv))
    subtypes.stats.optimal <- rbind(subtypes.stats.optimal,
                                    c(si,m.cur,ic.c,sprintf("%.1f +- %.1f",m.cur*100,ic.c*100)))
  }

  write.table(subtypes.stats.optimal,sep="\t",row.names=F,col.names=F,
              file=sprintf("%s.cv.optimal.k.stats.xls",output.tag))

  #############################
  ## Train the final model here
  sel.nbc.final <- train.function(D=D,  ## gene expression dataset
                              cl=cl, ## subtypes
                              GeneName=EntrezID, ## Entrez ID
                              w=w, ## This is weighted training
                              k=num.of.rules[optimal.k]) ## Need to set the optimal number of features obtained using cross-validation

  # aims.gs <- sel.nbc.final ## WE ARE GETTING the final model JUST HERE <-------
  # save(aims.gs,file=sprintf("%s.AIMS.GS.RData",output.tag))
  # 
  # sel.ids.nb <- get.all.pairs.genes(aims.gs$all.pairs)
  # 
  # symbol.pairs <- c()
  # for (pi in names(aims.gs$selected.pairs.list)){
  #   symbol.pairs <- rbind(symbol.pairs,cbind(pairs2symbols(aims.gs$selected.pairs.list[[pi]],
  #                                                          merged.dataset$probe.info,aims.gs$one.vs.all.tsp[[num.of.rules[optimal.k]]],pi),pi))
  # }
  # 
  # rownames(merged.dataset$probe.info) = merged.dataset$probe.info[,1]
  # nbc.to.mat <- plotNBClassifier(aims.gs$one.vs.all.tsp[[num.of.rules[optimal.k]]],merged.dataset$probe.info)
  # 
  # pdf(sprintf("%s.heatmap.nbc.pdf",output.tag),
  #     width=14, height=14)
  # heatmap.2(nbc.to.mat,scale="none",col=colorpanel(50, "white","red"),
  #           trace="none",margin=c(10,10),
  #           main="Simple rules conditional probabilities")
  # dev.off()
  # invisible(list(model=aims.gs,
  #                cv.stats=cv.stats,
  #                subtypes.stats.optimal=subtypes.stats.optimal))
  save(cv,sel.nbc.final,file=sprintf("%s_cv_and_final_model.RData",output.tag))
  list(cv_res=cv,final_model=sel.nbc.final,k=num.of.rules[optimal.k])
}
